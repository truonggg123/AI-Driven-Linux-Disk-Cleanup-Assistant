import os
import time
import pandas as pd
import numpy as np
import zipfile 
import sys
import joblib # Th∆∞ vi·ªán ƒë·ªÉ t·∫£i m√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán
from pathlib import Path
import spacy

# ==================== C·∫§U H√åNH V√Ä V·ªä TR√ç L∆ØU TR·ªÆ M√î H√åNH ====================
# Th∆∞ m·ª•c l∆∞u tr·ªØ m√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán (ƒê∆∞·ª£c t·∫°o b·ªüi train_model.py)
ASSETS_DIR = Path("ml_assets")
MODEL_FILE = ASSETS_DIR / "disk_model.joblib"
ENCODER_FILE = ASSETS_DIR / "label_encoder.joblib"
PCA_FILE = ASSETS_DIR / "pca_transformer.joblib"  # PCA transformer cho embedding vectors

# C√°c ng∆∞·ª°ng n√†y ƒë∆∞·ª£c d√πng ƒë·ªÉ t√≠nh to√°n Features (ƒë·∫∑c tr∆∞ng) tr√™n d·ªØ li·ªáu th·∫≠t.
# Ch√∫ng c·∫ßn ph·∫£i nh·∫•t qu√°n v·ªõi c√°c ng∆∞·ª°ng ƒë√£ d√πng trong train_model.py
DELETE_SIZE_THRESHOLD = 5 * 1024 * 1024   # 5MB
DELETE_TIME_THRESHOLD = 180               # 180 ng√†y

COMPRESS_SIZE_THRESHOLD = 50 * 1024 * 1024 # 50MB
COMPRESS_TIME_THRESHOLD = 90              # 90 ng√†y
COMPRESSED_EXTS = [".zip", ".rar", ".7z", ".tar.gz", ".gz"]
# ==============================================================================

# ----------------- H√ÄM TR√çCH XU·∫§T FEATURES T·ª™ T√äN FILE B·∫∞NG SPACY -----------------
def extract_spacy_features(file_path, nlp_model):
    """
    Tr√≠ch xu·∫•t features t·ª´ t√™n file s·ª≠ d·ª•ng spaCy NLP.
    Tr·∫£ v·ªÅ: dict ch·ª©a c√°c features t·ª´ NLP
    """
    # L·∫•y t√™n file kh√¥ng c√≥ extension
    file_name = Path(file_path).stem.lower()
    
    # X·ª≠ l√Ω b·∫±ng spaCy
    doc = nlp_model(file_name)
    
    # Feature 1: S·ªë l∆∞·ª£ng t·ª´ trong t√™n file
    num_words = len([token for token in doc if token.is_alpha])
    
    # Feature 2: ƒê·ªô d√†i t√™n file
    name_length = len(file_name)
    
    # Feature 3: C√≥ ch·ª©a t·ª´ kh√≥a temp/old/backup kh√¥ng
    temp_keywords = ['temp', 'tmp', 'cache', 'old', 'backup', 'bak', '~']
    has_temp_keyword = any(keyword in file_name for keyword in temp_keywords)
    
    # Feature 4: C√≥ ch·ª©a t·ª´ kh√≥a quan tr·ªçng kh√¥ng
    important_keywords = ['important', 'final', 'document', 'report']
    has_important_keyword = any(keyword in file_name for keyword in important_keywords)
    
    # Feature 5: Embedding vector t·ª´ t√™n file (gi·∫£m chi·ªÅu b·∫±ng PCA sau)
    if len(doc) > 0 and doc.vector is not None:
        embedding_vector = doc.vector  # Vector 300D t·ª´ en_core_web_lg
    else:
        embedding_vector = np.zeros(300)  # Vector 0 n·∫øu kh√¥ng c√≥ token
    
    return {
        'num_words': num_words,
        'name_length': name_length,
        'has_temp_keyword': int(has_temp_keyword),
        'has_important_keyword': int(has_important_keyword),
        'embedding_vector': embedding_vector
    }

# ----------------- H√ÄM THU TH·∫¨P METADATA TH·∫¨T -----------------
def collect_real_metadata(target_dir_path):
    """Qu√©t th∆∞ m·ª•c th·ª±c, thu th·∫≠p metadata c·ªßa c√°c t·ªáp (tr·ª´ th∆∞ m·ª•c)."""
    
    current_time = time.time()
    file_data_list = []
    total_files = 0
    
    print(f"\n[B∆Ø·ªöC 2] B·∫Øt ƒë·∫ßu qu√©t th∆∞ m·ª•c th·ª±c: {target_dir_path.resolve()}")
    
    # S·ª≠ d·ª•ng rglob ƒë·ªÉ qu√©t ƒë·ªá quy (bao g·ªìm c√°c th∆∞ m·ª•c con)
    for item_path in target_dir_path.rglob('*'):
        if item_path.is_file():
            total_files += 1
            try:
                stat_info = item_path.stat()
                
                if item_path.is_symlink():
                    continue

                size_bytes = stat_info.st_size
                # st_atime: th·ªùi gian truy c·∫≠p cu·ªëi c√πng (last access time)
                days_since_access = (current_time - stat_info.st_atime) / (24 * 3600)
                
                # B·ªè qua c√°c t·ªáp qu√° m·ªõi (< 7 ng√†y)
                if days_since_access < 7: continue

                file_data_list.append({
                    'file_path': item_path.as_posix(),
                    'size_bytes': size_bytes,
                    'extension': item_path.suffix.lower(),
                    'days_since_access': days_since_access,
                })

            except Exception as e:
                print(f"L·ªói khi x·ª≠ l√Ω {item_path}: {e}")
                continue

    print(f"Ho√†n t·∫•t qu√©t. Thu th·∫≠p ƒë∆∞·ª£c {len(file_data_list)}/{total_files} t·ªáp h·ª£p l·ªá.")
    return pd.DataFrame(file_data_list)

# -------------------- H√ÄM CH·ªà T√çNH TO√ÅN FEATURE --------------------
def calculate_features(df, nlp_model, pca_transformer):
    """
    T√≠nh to√°n Features (ƒë·∫∑c tr∆∞ng) c·∫ßn thi·∫øt cho m√¥ h√¨nh.
    Bao g·ªìm c√°c features t·ª´ spaCy gi·ªëng nh∆∞ trong train_model.py
    """
    if df.empty:
        return df

    # ƒê·∫∑c tr∆∞ng 1: L√† file t·∫°m th·ªùi/r√°c hay kh√¥ng
    temp_extensions = [".log", ".tmp", ".bak", ".cache", ".~", ""]
    df['is_temp_file'] = df['extension'].isin(temp_extensions).astype(int)

    # ƒê·∫∑c tr∆∞ng 2: K√≠ch th∆∞·ªõc t·ªáp (d√πng log scale nh∆∞ khi train)
    df['size_log'] = np.log10(df['size_bytes'] + 1)
    
    # ƒê·∫∑c tr∆∞ng 3: Th·ªùi gian k·ªÉ t·ª´ l·∫ßn truy c·∫≠p cu·ªëi c√πng (days_since_access)
    
    # Tr√≠ch xu·∫•t features t·ª´ t√™n file b·∫±ng spaCy
    print("   ƒêang tr√≠ch xu·∫•t features t·ª´ t√™n file b·∫±ng spaCy...")
    spacy_features = []
    embedding_vectors = []
    
    for idx, file_path in enumerate(df['file_path']):
        features = extract_spacy_features(file_path, nlp_model)
        spacy_features.append({
            'num_words': features['num_words'],
            'name_length': features['name_length'],
            'has_temp_keyword': features['has_temp_keyword'],
            'has_important_keyword': features['has_important_keyword']
        })
        embedding_vectors.append(features['embedding_vector'])
        
        if (idx + 1) % 1000 == 0:
            print(f"   ƒê√£ x·ª≠ l√Ω {idx + 1}/{len(df)} t·ªáp...")
    
    # Th√™m c√°c features t·ª´ spaCy v√†o DataFrame
    spacy_df = pd.DataFrame(spacy_features)
    df = pd.concat([df, spacy_df], axis=1)
    
    # Gi·∫£m chi·ªÅu embedding vectors b·∫±ng PCA ƒë√£ ƒë∆∞·ª£c hu·∫•n luy·ªán
    embedding_matrix = np.array(embedding_vectors)
    embedding_reduced = pca_transformer.transform(embedding_matrix)
    
    # T·∫°o t√™n c·ªôt cho embedding features
    embedding_cols = [f'embedding_dim_{i}' for i in range(embedding_reduced.shape[1])]
    embedding_df = pd.DataFrame(embedding_reduced, columns=embedding_cols, index=df.index)
    
    # K·∫øt h·ª£p t·∫•t c·∫£ features
    df = pd.concat([df, embedding_df], axis=1)
    
    return df

# ----------------- H√ÄM ƒê·ªäNH D·∫†NG K√çCH TH∆Ø·ªöC -----------------
def format_size(size_bytes):
    """Chuy·ªÉn ƒë·ªïi k√≠ch th∆∞·ªõc t·ªáp t·ª´ bytes sang ƒë·ªãnh d·∫°ng d·ªÖ ƒë·ªçc (KB, MB, GB)."""
    if size_bytes >= (1024**3):
        return f"{size_bytes / (1024**3):.2f} GB"
    elif size_bytes >= (1024**2):
        return f"{size_bytes / (1024**2):.2f} MB"
    else:
        return f"{size_bytes / 1024:.2f} KB"

# ----------------- H√ÄM T∆Ø∆†NG T√ÅC V√Ä TH·ª∞C THI H√ÄNH ƒê·ªòNG -----------------
def confirm_and_act(suggestions_df, target_dir):
    """H·ªèi ng∆∞·ªùi d√πng v√† th·ª±c hi·ªán h√†nh ƒë·ªông TH·ª∞C T·∫æ."""
    
    if suggestions_df.empty:
        return

    # L·∫•y nh√£n d·ª± ƒëo√°n cho h√†nh ƒë·ªông hi·ªán t·∫°i (Delete ho·∫∑c Compress)
    action_type = suggestions_df['Predicted_Label'].iloc[0]
    
    print(f"\nB·∫°n c√≥ mu·ªën th·ª±c hi·ªán h√†nh ƒë·ªông '{action_type}' tr√™n {len(suggestions_df)} t·ªáp n√†y kh√¥ng?")
    
    # Hi·ªÉn th·ªã th√¥ng tin t·ªáp cho ng∆∞·ªùi d√πng
    display_cols = ['file_path', 'Formatted_Size', 'days_since_access']
    print(suggestions_df[display_cols].to_string(index=False))
    
    response = input("Nh·∫≠p 'y' ƒë·ªÉ x√°c nh·∫≠n th·ª±c hi·ªán ho·∫∑c b·∫•t k·ª≥ ph√≠m n√†o kh√°c ƒë·ªÉ b·ªè qua: ").lower()
    
    if response == 'y':
        print(f"\n--- Th·ª±c hi·ªán {action_type} TH·ª∞C T·∫æ ---")
        for index, row in suggestions_df.iterrows():
            file_path = Path(row['file_path'])
            
            if action_type == 'Delete':
                try:
                    file_path.unlink() # TH·ª∞C HI·ªÜN X√ìA T·ªÜP TH·∫¨T S·ª∞
                    print(f"   [X√ìA TH·ª∞C T·∫æ] ƒê√£ x√≥a: {file_path.name}")
                except Exception as e:
                    print(f"   [L·ªñI X√ìA] Kh√¥ng th·ªÉ x√≥a {file_path.name}: {e}")

            elif action_type == 'Compress':
                # T·∫°o th∆∞ m·ª•c Archive
                archive_dir = target_dir / "ARCHIVE_ML_ASSISTANT"
                archive_dir.mkdir(exist_ok=True)
                
                zip_path = archive_dir / f"{file_path.stem}.zip"
                
                try:
                    # TH·ª∞C HI·ªÜN N√âN T·ªÜP TH·∫¨T S·ª∞
                    print(f"   [N√âN TH·ª∞C T·∫æ] ƒêang n√©n {file_path.name}...")
                    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
                        zipf.write(file_path, file_path.name) 
                    
                    # X√≥a t·ªáp g·ªëc sau khi n√©n th√†nh c√¥ng
                    file_path.unlink()
                    print(f"   [HO√ÄN T·∫§T] ƒê√£ n√©n v√† x√≥a g·ªëc: {file_path.name}")
                    
                except Exception as e:
                    print(f"   [L·ªñI N√âN] Kh√¥ng th·ªÉ n√©n {file_path.name}: {e}")

        print(f"Ho√†n t·∫•t h√†nh ƒë·ªông '{action_type}'.")
    else:
        print("H·ªßy b·ªè h√†nh ƒë·ªông.")


# ========================= H√ÄM CH√çNH (MAIN FUNCTION) =========================
def main():
    
    print("================== TR·ª¢ L√ù D·ªåN D·∫∏P ƒêƒ®A ML - ·ª®NG D·ª§NG ===================")
    
    # -------------------------------------------------------------
    # B∆Ø·ªöC 0: T·∫¢I M√î H√åNH, B·ªò M√É H√ìA, PCA V√Ä SPACY MODEL
    # -------------------------------------------------------------
    try:
        model = joblib.load(MODEL_FILE)
        le = joblib.load(ENCODER_FILE)
        pca = joblib.load(PCA_FILE)
        print(f"[B∆Ø·ªöC 0] ƒê√£ t·∫£i m√¥ h√¨nh t·ª´: {MODEL_FILE}")
        print(f"[B∆Ø·ªöC 0] ƒê√£ t·∫£i PCA transformer t·ª´: {PCA_FILE}")
    except FileNotFoundError as e:
        print("\n[L·ªñI QUAN TR·ªåNG] Kh√¥ng t√¨m th·∫•y file m√¥ h√¨nh ho·∫∑c transformer!")
        print(f"Vui l√≤ng ch·∫°y file 'train_model.py' tr∆∞·ªõc ƒë·ªÉ t·∫°o c√°c file c·∫ßn thi·∫øt.")
        print(f"Chi ti·∫øt l·ªói: {e}")
        sys.exit(1)
    
    # Kh·ªüi t·∫°o spaCy model
    try:
        nlp = spacy.load("en_core_web_lg")
        print("[B∆Ø·ªöC 0] ƒê√£ t·∫£i m√¥ h√¨nh spaCy: en_core_web_lg")
    except OSError:
        print("[L·ªñI] Kh√¥ng t√¨m th·∫•y m√¥ h√¨nh spaCy 'en_core_web_lg'.")
        print("Vui l√≤ng ch·∫°y: python -m spacy download en_core_web_lg")
        sys.exit(1)
    
    # -------------------------------------------------------------
    # B∆Ø·ªöC 1: X√ÅC ƒê·ªäNH TH∆Ø M·ª§C TH·∫¨T T·∫æ
    # -------------------------------------------------------------
    while True:
        target_path_str = input("\n[B∆Ø·ªöC 1] Nh·∫≠p ƒë∆∞·ªùng d·∫´n th∆∞ m·ª•c C·∫¶N D·ªåN D·∫∏P (V√≠ d·ª•: /home/user/Downloads): ")
        target_dir = Path(target_path_str)
        if target_dir.is_dir():
            break
        else:
            print("ƒê∆∞·ªùng d·∫´n kh√¥ng h·ª£p l·ªá ho·∫∑c kh√¥ng ph·∫£i l√† th∆∞ m·ª•c. Vui l√≤ng th·ª≠ l·∫°i.")
            
    # -------------------------------------------------------------
    # B∆Ø·ªöC 2: THU TH·∫¨P D·ªÆ LI·ªÜU TH·∫¨T & T√çNH TO√ÅN FEATURE
    # -------------------------------------------------------------
    real_metadata_df = collect_real_metadata(target_dir)

    if real_metadata_df.empty:
        print("Kh√¥ng t√¨m th·∫•y t·ªáp n√†o ƒë·ªß ƒëi·ªÅu ki·ªán ƒë·ªÉ ph√¢n t√≠ch. Tho√°t ch∆∞∆°ng tr√¨nh.")
        sys.exit(0) 
        
    real_df = calculate_features(real_metadata_df, nlp, pca) 
    
    # -------------------------------------------------------------
    # B∆Ø·ªöC 3: D·ª∞ ƒêO√ÅN & B√ÅO C√ÅO
    # -------------------------------------------------------------
    print("\n[B∆Ø·ªöC 3] √Åp d·ª•ng m√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán ƒë·ªÉ d·ª± ƒëo√°n h√†nh ƒë·ªông tr√™n d·ªØ li·ªáu th·∫≠t...")
    
    # Ch·ªçn c√°c c·ªôt feature m√† m√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c hu·∫•n luy·ªán (gi·ªëng nh∆∞ khi train)
    feature_cols = ['size_log', 'days_since_access', 'is_temp_file',
                    'num_words', 'name_length', 'has_temp_keyword', 'has_important_keyword']
    # Th√™m c√°c embedding dimensions
    embedding_cols = [f'embedding_dim_{i}' for i in range(10)]
    feature_cols.extend(embedding_cols)
    
    # Ch·ªâ l·∫•y c√°c c·ªôt c√≥ trong DataFrame (tr√°nh l·ªói n·∫øu thi·∫øu)
    available_cols = [col for col in feature_cols if col in real_df.columns]
    X_real = real_df[available_cols]
    
    # D·ª± ƒëo√°n
    all_predictions_encoded = model.predict(X_real)
    real_df['Predicted_Label'] = le.inverse_transform(all_predictions_encoded)
    
    # Ph√¢n lo·∫°i k·∫øt qu·∫£
    action_df = real_df[real_df['Predicted_Label'] != 'Keep'].copy()
    action_df['Formatted_Size'] = action_df['size_bytes'].apply(format_size)

    delete_suggestions = action_df[action_df['Predicted_Label'] == 'Delete'] \
        .sort_values(by='size_bytes', ascending=False)
    compress_suggestions = action_df[action_df['Predicted_Label'] == 'Compress'] \
        .sort_values(by='size_bytes', ascending=False)

    print("\n=============== B√ÅO C√ÅO ƒê·ªÄ XU·∫§T D·ªåN D·∫∏P ===============")
    
    # B√°o c√°o X√≥a
    print("\n--- üóëÔ∏è ƒê·ªÅ Xu·∫•t X√ìA (Delete) ---")
    if not delete_suggestions.empty:
        print(f"T·ªïng s·ªë t·ªáp ƒë·ªÅ xu·∫•t x√≥a: {len(delete_suggestions)}")
        print("Danh s√°ch TOP 5 t·ªáp c·∫ßn x√≥a (theo k√≠ch th∆∞·ªõc):")
        print(delete_suggestions.head(5).to_string(columns=['file_path', 'Formatted_Size', 'days_since_access'], index=False))
        confirm_and_act(delete_suggestions.head(5), target_dir) 
    else:
        print("Kh√¥ng c√≥ t·ªáp n√†o ƒë∆∞·ª£c ƒë·ªÅ xu·∫•t x√≥a.")

    # B√°o c√°o N√©n
    print("\n--- üì¶ ƒê·ªÅ Xu·∫•t N√âN/L∆ØU TR·ªÆ (Compress) ---")
    if not compress_suggestions.empty:
        print(f"T·ªïng s·ªë t·ªáp ƒë·ªÅ xu·∫•t n√©n: {len(compress_suggestions)}")
        print("Danh s√°ch TOP 5 t·ªáp c·∫ßn n√©n:")
        print(compress_suggestions.head(5).to_string(columns=['file_path', 'Formatted_Size', 'days_since_access'], index=False))
        confirm_and_act(compress_suggestions.head(5), target_dir)
    else:
        print("Kh√¥ng c√≥ t·ªáp n√†o ƒë∆∞·ª£c ƒë·ªÅ xu·∫•t n√©n.")

    print("\n================== CH∆Ø∆†NG TR√åNH K·∫æT TH√öC ==================")

if __name__ == '__main__':
    main()
